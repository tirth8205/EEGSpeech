Metadata-Version: 2.4
Name: eegspeech
Version: 2.0.0
Summary: Advanced healthcare BCI with VLM integration for clinical EEG analysis and speech decoding
Home-page: https://github.com/yourusername/eegspeech
Author: Your Name
Author-email: your.email@example.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Bio-Informatics
Requires-Python: >=3.8,<3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy<2.0.0,>=1.26.0
Requires-Dist: torch<3.0.0,>=2.2.0
Requires-Dist: matplotlib<4.0.0,>=3.10.0
Requires-Dist: scikit-learn<2.0.0,>=1.6.0
Requires-Dist: streamlit<2.0.0,>=1.44.0
Requires-Dist: plotly<7.0.0,>=6.0.0
Requires-Dist: scipy<2.0.0,>=1.15.0
Requires-Dist: mne<2.0.0,>=1.8.0
Requires-Dist: seaborn<0.14.0,>=0.13.0
Requires-Dist: torchinfo<2.0.0,>=1.8.0
Requires-Dist: pandas<3.0.0,>=2.2.0
Requires-Dist: pillow<12.0.0,>=11.0.0
Requires-Dist: altair<6.0.0,>=5.5.0
Requires-Dist: pyarrow<21.0.0,>=20.0.0
Requires-Dist: pydeck<0.10.0,>=0.9.0
Requires-Dist: requests<3.0.0,>=2.32.0
Requires-Dist: jinja2<4.0.0,>=3.1.0
Requires-Dist: transformers<5.0.0,>=4.36.0
Requires-Dist: opencv-python<5.0.0,>=4.8.0
Requires-Dist: openai<2.0.0,>=1.0.0
Requires-Dist: accelerate<1.0.0,>=0.25.0
Requires-Dist: timm<1.0.0,>=0.9.0
Provides-Extra: dev
Requires-Dist: pytest<9.0.0,>=8.0.0; extra == "dev"
Requires-Dist: black<25.0.0,>=24.0.0; extra == "dev"
Requires-Dist: flake8<8.0.0,>=7.0.0; extra == "dev"
Provides-Extra: vlm
Requires-Dist: transformers<5.0.0,>=4.36.0; extra == "vlm"
Requires-Dist: opencv-python<5.0.0,>=4.8.0; extra == "vlm"
Requires-Dist: openai<2.0.0,>=1.0.0; extra == "vlm"
Requires-Dist: accelerate<1.0.0,>=0.25.0; extra == "vlm"
Requires-Dist: timm<1.0.0,>=0.9.0; extra == "vlm"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# EEGSpeech: Neural Speech Decoding from EEG Signals

![Python](https://img.shields.io/badge/python-3.8%20|%203.9%20|%203.10-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Docker](https://img.shields.io/badge/docker-supported-blue)
![Status](https://img.shields.io/badge/status-active-brightgreen)

A robust brain-computer interface (BCI) for decoding speech phonemes from EEG signals using a hybrid CNN-LSTM model. Supports synthetic and real EEG data, with interactive visualizations and containerized deployment for research and production.

## 📌 Project Overview

**Objective**: Decode 8 speech phonemes (/a/, /e/, /i/, /o/, /u/, /p/, /t/, /k/) from EEG signals using deep learning.

**Key Features**:
- Hybrid CNN-LSTM model for accurate phoneme classification.
- Synthetic EEG data with vowel and consonant patterns, plus real EEG (EDF) support.
- Streamlit app with topographic scalp maps, Grad-CAM, and performance metrics.
- Robust training with k-fold cross-validation and metrics (accuracy, F1-score).
- Dockerized for consistent deployment.
- CLI for training, evaluation, prediction, and analysis.
- Grad-CAM for model interpretability.

## 🗂 Project Structure

```
eegspeech/
├── eegspeech/
│   ├── app/
│   │   ├── app.py        # Streamlit web application
│   │   └── cli.py        # Command-line interface
│   ├── models/
│   │   ├── dataset.py    # Data generation and preprocessing
│   │   ├── model.py      # CNN-LSTM model
│   │   ├── train.py      # Training with k-fold
│   │   └── utils.py      # Visualization utilities
├── Dockerfile            # Containerized deployment
├── requirements.txt      # Dependencies
├── setup.py             # Package configuration
├── README.md            # Documentation
├── LICENSE              # MIT License
├── CONTRIBUTING.md      # Contribution guidelines
```

## 🚀 Installation

### Prerequisites
- Python 3.8–3.10
- Git
- Docker (optional, for containerized deployment)

### Local Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/tirth8205/EEGSpeech.git
   cd EEGSpeech
   ```
2. Create and activate a virtual environment:
   ```bash
   python3.10 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   pip install -e .
   ```

### Docker Installation
1. Build the Docker image:
   ```bash
   docker build -t eegspeech .
   ```
2. Run the Streamlit app:
   ```bash
   docker run -p 8501:8501 eegspeech
   ```
   Access at `http://localhost:8501`.
3. Run CLI commands (e.g., training with persistent model storage):
   ```bash
   docker run -v $(pwd)/models:/app/models eegspeech eegspeech train --epochs 50 --output models/eeg_speech_classifier.pth
   ```

## 🧠 Usage

### Training the Model
Train with synthetic data and k-fold cross-validation:
```bash
eegspeech train --epochs 50 --batch-size 32 --lr 0.001 --kfold
```
Train with real EEG data:
```bash
eegspeech train --data-type real --file-path path/to/eeg.edf --output models/model.pth
```

**Example Output**:
```
Training with lr=0.001, batch_size=64
...
Early stopping at epoch 16
Test Accuracy: 0.9267
Test Precision: 0.9295
Test Recall: 0.9267
Test F1-Score: 0.9253
Model saved to eeg_speech_classifier.pth
Best parameters: lr=0.001, batch_size=64
```

### Running the Streamlit App
```bash
streamlit run eegspeech/app/app.py
```
**Features**:
- Upload real EEG data (EDF) or generate synthetic samples.
- Visualize EEG signals, topographic maps, and Grad-CAM.
- View training history and confusion matrix.
- Select channels and time ranges interactively.

### Making Predictions
Predict phonemes from an EDF file:
```bash
eegspeech predict --input-file path/to/eeg.edf --output-file predictions.txt
```

### Analyzing the Model
Check model complexity and generate Grad-CAM:
```bash
eegspeech analyze --grad-cam
```

## 🧬 Technical Details

### Model Architecture
- **Hybrid CNN-LSTM**:
  - 3-layer CNN (32→64→128 filters) with batch normalization.
  - 2-layer bidirectional LSTM (256 hidden units).
  - Classifier (512→128→8).
  - ~1.5M parameters, ~500M FLOPs.
- **Regularization**: Dropout (0.4), weight decay (0.001).

### Data Processing
- **Synthetic**: 14 channels, 1000 Hz, with vowel (/a/, /e/, /i/, /o/, /u/) and consonant (/p/, /t/, /k/) patterns, augmented with noise and time warping.
- **Real EEG**: EDF files, 1–50 Hz band-pass filtering, 1-second epochs.

### Training Pipeline
- **Optimizer**: Adam (tuned learning rate).
- **Loss**: CrossEntropy.
- **Validation**: 5-fold cross-validation.
- **Metrics**: Accuracy, precision, recall, F1-score, confusion matrix.

## 🔮 Potential Extensions
- Integrate with BCI headsets (e.g., OpenBCI).
- Expand to syllable or word decoding.
- Deploy as a FastAPI service.
- Support real-time EEG streaming.

## 📚 References
- Jayalath, D., Landau, G., Shillingford, B., Woolrich, M., & Parker Jones, O. (2024). "The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning." *arXiv preprint arXiv:2406.04328*. https://arxiv.org/abs/2406.04328
- MNE-Python for EEG processing: https://mne.tools
- PyTorch for deep learning: https://pytorch.org
- Streamlit for UI: https://streamlit.io

## 🤝 Contributing
See `CONTRIBUTING.md` for guidelines. Issues and PRs welcome on GitHub.

## 📜 License
MIT License. See `LICENSE` for details.

## 📸 Screenshots

Below are screenshots of the Streamlit app showcasing the EEG speech decoding interface:

![Streamlit App - Main Interface](https://raw.githubusercontent.com/tirth8205/EEGSpeech/main/screenshot/Screenshot%202025-04-29%20at%2022.18.48.png)

![Streamlit App - Additional View](https://raw.githubusercontent.com/tirth8205/EEGSpeech/main/screenshot/Screenshot%202025-04-29%20at%2022.19.22.png)
